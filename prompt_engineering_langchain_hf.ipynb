{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T08:39:45.997677Z",
     "start_time": "2025-07-25T08:39:43.935488Z"
    }
   },
   "source": [
    "from langchain_huggingface import ChatHuggingFace #to use huggingface llm class directly\n",
    "from langchain_huggingface import HuggingFaceEndpoint #to run models serverless manner\n",
    "from langchain_huggingface import HuggingFacePipeline #to run models locally\n",
    "from langchain_huggingface import HuggingFaceEmbeddings #to run embedding model through dedicated endpoints\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings #to run embedding models serverless manner\n",
    "from langchain_huggingface import HuggingFaceEmbeddings #to run embedding models locally\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings #to run embedding models locally\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings #to run bge embedding model"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:06:22.696706Z",
     "start_time": "2025-07-23T08:06:22.676627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login() # You will be prompted for your HF key, which will then be saved locally"
   ],
   "id": "a772fb734478e6a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f0ee044392640228d6732f6185c18fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:02:03.809361Z",
     "start_time": "2025-07-25T09:02:03.799341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=10,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ],
   "id": "afbce66f10abbc9e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:01:33.127307Z",
     "start_time": "2025-07-25T09:01:33.018468Z"
    }
   },
   "cell_type": "code",
   "source": "res = llm.invoke(\"Who is the PM of India?\")",
   "id": "a3f71717def6656d",
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m res \u001B[38;5;241m=\u001B[39m llm\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWho is the PM of India?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    386\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    387\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 389\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    390\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    391\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    392\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    393\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    394\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    395\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    396\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    397\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    398\u001B[0m         )\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m    401\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    757\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    758\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    759\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    763\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    764\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    765\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 766\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    957\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    958\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    959\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    969\u001B[0m         )\n\u001B[0;32m    970\u001B[0m     ]\n\u001B[1;32m--> 971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[0;32m    972\u001B[0m         prompts,\n\u001B[0;32m    973\u001B[0m         stop,\n\u001B[0;32m    974\u001B[0m         run_managers,\n\u001B[0;32m    975\u001B[0m         new_arg_supported\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(new_arg_supported),\n\u001B[0;32m    976\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    977\u001B[0m     )\n\u001B[0;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    979\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    980\u001B[0m         callback_managers[idx]\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    981\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[0;32m    989\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    783\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    788\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    789\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    791\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 792\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    793\u001B[0m                 prompts,\n\u001B[0;32m    794\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    795\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[0;32m    796\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    797\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    798\u001B[0m             )\n\u001B[0;32m    799\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    800\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    801\u001B[0m         )\n\u001B[0;32m    802\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    803\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1544\u001B[0m, in \u001B[0;36mLLM._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1542\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[0;32m   1543\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m-> 1544\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1545\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m   1546\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1547\u001B[0m     )\n\u001B[0;32m   1548\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001B[0m, in \u001B[0;36mHuggingFaceEndpoint._call\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m completion\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m     response_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mtext_generation(\n\u001B[0;32m    313\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m    314\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minvocation_params,\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001B[39;00m\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001B[39;00m\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m stop_seq \u001B[38;5;129;01min\u001B[39;00m invocation_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2296\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[0;32m   2290\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2291\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI endpoint/model for text-generation is not served via TGI. Cannot return output as a stream.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2292\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please pass `stream=False` as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2293\u001B[0m         )\n\u001B[0;32m   2295\u001B[0m model_id \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m-> 2296\u001B[0m provider_helper \u001B[38;5;241m=\u001B[39m get_provider_helper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprovider, task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mmodel_id)\n\u001B[0;32m   2297\u001B[0m request_parameters \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39mprepare_request(\n\u001B[0;32m   2298\u001B[0m     inputs\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m   2299\u001B[0m     parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2303\u001B[0m     api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken,\n\u001B[0;32m   2304\u001B[0m )\n\u001B[0;32m   2306\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\__init__.py:191\u001B[0m, in \u001B[0;36mget_provider_helper\u001B[1;34m(provider, task, model)\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecifying a model is required when provider is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    190\u001B[0m     provider_mapping \u001B[38;5;241m=\u001B[39m _fetch_inference_provider_mapping(model)\n\u001B[1;32m--> 191\u001B[0m     provider \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(provider_mapping))\u001B[38;5;241m.\u001B[39mprovider\n\u001B[0;32m    193\u001B[0m provider_tasks \u001B[38;5;241m=\u001B[39m PROVIDERS\u001B[38;5;241m.\u001B[39mget(provider)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m provider_tasks \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mStopIteration\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T17:03:33.670368Z",
     "start_time": "2025-07-22T17:03:33.659298Z"
    }
   },
   "cell_type": "code",
   "source": "chat = ChatHuggingFace(llm=llm, verbose=True)",
   "id": "66c00d9e633909dc",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T17:06:31.370645Z",
     "start_time": "2025-07-22T17:06:31.251895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ],
   "id": "97a7744587f42fb1",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model microsoft/phi-2 is not supported for task conversational and provider featherless-ai. Supported task: text-generation.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      2\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are a helpful translator. Translate the user \u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124m    sentence to French.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      4\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI love programming.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      5\u001B[0m ]\n\u001B[1;32m----> 7\u001B[0m chat\u001B[38;5;241m.\u001B[39minvoke(messages)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:378\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    374\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    375\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m--> 378\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    379\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    380\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    381\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    382\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    383\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    384\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    385\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    386\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    387\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    388\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:963\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    955\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    956\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    960\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    961\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    962\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:782\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    781\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 782\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    783\u001B[0m                 m,\n\u001B[0;32m    784\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    785\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    786\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    787\u001B[0m             )\n\u001B[0;32m    788\u001B[0m         )\n\u001B[0;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1028\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1028\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1029\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1030\u001B[0m     )\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1032\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:574\u001B[0m, in \u001B[0;36mChatHuggingFace._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    567\u001B[0m     message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    568\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    569\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[0;32m    570\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    571\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream} \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m    572\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    573\u001B[0m     }\n\u001B[1;32m--> 574\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mchat_completion(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(answer)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:917\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001B[0m\n\u001B[0;32m    895\u001B[0m \u001B[38;5;66;03m# Prepare the payload\u001B[39;00m\n\u001B[0;32m    896\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    897\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: payload_model,\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    915\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(extra_body \u001B[38;5;129;01mor\u001B[39;00m {}),\n\u001B[0;32m    916\u001B[0m }\n\u001B[1;32m--> 917\u001B[0m request_parameters \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39mprepare_request(\n\u001B[0;32m    918\u001B[0m     inputs\u001B[38;5;241m=\u001B[39mmessages,\n\u001B[0;32m    919\u001B[0m     parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    920\u001B[0m     headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    921\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_id_or_url,\n\u001B[0;32m    922\u001B[0m     api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken,\n\u001B[0;32m    923\u001B[0m )\n\u001B[0;32m    924\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_post(request_parameters, stream\u001B[38;5;241m=\u001B[39mstream)\n\u001B[0;32m    926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:93\u001B[0m, in \u001B[0;36mTaskProviderHelper.prepare_request\u001B[1;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001B[0m\n\u001B[0;32m     90\u001B[0m api_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_api_key(api_key)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# mapped model from HF model ID\u001B[39;00m\n\u001B[1;32m---> 93\u001B[0m provider_mapping_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_mapping_info(model)\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001B[39;00m\n\u001B[0;32m     96\u001B[0m headers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_headers(headers, api_key)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:162\u001B[0m, in \u001B[0;36mTaskProviderHelper._prepare_mapping_info\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported by provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprovider\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m provider_mapping\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask:\n\u001B[1;32m--> 162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported for task \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprovider\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupported task: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprovider_mapping\u001B[38;5;241m.\u001B[39mtask\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    165\u001B[0m     )\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m provider_mapping\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaging\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    168\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is in staging mode for provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprovider\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Meant for test purposes only.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    170\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Model microsoft/phi-2 is not supported for task conversational and provider featherless-ai. Supported task: text-generation."
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:16:51.972840Z",
     "start_time": "2025-07-25T09:16:23.142696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch # Still needed for device settings if not using default CPU\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-3-mini-4k-instruct\", #\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    device_map=\"cpu\", # Automatically use GPU if available\n",
    "    # torch_dtype=torch.bfloat16, # For memory efficiency\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 10,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        # For Phi-3: \"trust_remote_code\": True, # This might go into model_kwargs if not directly supported by pipeline_kwargs\n",
    "    },\n",
    "    # You can also pass model_kwargs for arguments directly to model.from_pretrained\n",
    "    # model_kwargs={\"trust_remote_code\": True},\n",
    ")"
   ],
   "id": "4a10d7462645c898",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "217b3a60338d449f9728700e49357e04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:20:08.906309Z",
     "start_time": "2025-07-23T12:17:03.911823Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(\"Who is the PM of India?\")",
   "id": "4789cb3847b150cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the PM of India?\\nOptions:\\n(A) Narendra Modi (B) Rahul Gandhi (C) Amit Shah (D) Sonia Gandhi. The correct answer is (A) Narendra Modi, who has been serving as Prime Minister since May 26, 2014. He was re-elected for a second term in April 2019 and continues to hold office at present.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:15:38.147846Z",
     "start_time": "2025-07-25T09:15:37.324341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "\n",
    "messages = \"<system>You are a helpful translator. Translate the user sentence to French.\\n <human>I love programming.\"\n",
    "# messages = [\n",
    "#     (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "#     (\"human\", \"I love programming.\"),\n",
    "# ]\n",
    "\n",
    "chat.invoke(messages)"
   ],
   "id": "c69fa943f8938e9",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 9\u001B[0m\n\u001B[0;32m      3\u001B[0m messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<system>You are a helpful translator. Translate the user sentence to French.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m <human>I love programming.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# messages = [\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#     (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#     (\"human\", \"I love programming.\"),\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# ]\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m chat\u001B[38;5;241m.\u001B[39minvoke(messages)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:378\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    374\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    375\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m--> 378\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    379\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    380\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    381\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    382\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    383\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    384\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    385\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    386\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    387\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    388\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:963\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    955\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    956\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    960\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    961\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    962\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:782\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    781\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 782\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    783\u001B[0m                 m,\n\u001B[0;32m    784\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    785\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    786\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    787\u001B[0m             )\n\u001B[0;32m    788\u001B[0m         )\n\u001B[0;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1028\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1028\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1029\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1030\u001B[0m     )\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1032\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:577\u001B[0m, in \u001B[0;36mChatHuggingFace._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(answer)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 577\u001B[0m     llm_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_chat_prompt(messages)\n\u001B[0;32m    579\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_stream:\n\u001B[0;32m    580\u001B[0m         stream_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39m_stream(\n\u001B[0;32m    581\u001B[0m             llm_input, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    582\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:731\u001B[0m, in \u001B[0;36mChatHuggingFace._to_chat_prompt\u001B[1;34m(self, messages)\u001B[0m\n\u001B[0;32m    727\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLast message must be a HumanMessage!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    729\u001B[0m messages_dicts \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_chatml_format(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n\u001B[1;32m--> 731\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[0;32m    732\u001B[0m     messages_dicts, tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    733\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1621\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.apply_chat_template\u001B[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1619\u001B[0m     tokenizer_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1621\u001B[0m chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_chat_template(chat_template, tools)\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(conversation, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[0;32m   1624\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(conversation[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(conversation[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1625\u001B[0m ):\n\u001B[0;32m   1626\u001B[0m     conversations \u001B[38;5;241m=\u001B[39m conversation\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1743\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.get_chat_template\u001B[1;34m(self, chat_template, tools)\u001B[0m\n\u001B[0;32m   1741\u001B[0m         chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_template\n\u001B[0;32m   1742\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1744\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1745\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margument was passed! For information about writing templates and setting the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1746\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1747\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1748\u001B[0m         )\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chat_template\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:09:55.015567Z",
     "start_time": "2025-07-25T09:09:43.764935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#alternativ we can make the hf pipeline and pass it through HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ],
   "id": "15a9838a0d1ea750",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritap\\anaconda3\\envs\\data_science\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1582: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce7f85c7f96d471588557fc87d94b502"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:10:06.761566Z",
     "start_time": "2025-07-25T09:09:55.050601Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(\"Who is the PM of India?\")",
   "id": "48bc0d5a37b199af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Who is the PM of India?\\nAnswer: Narendra Modi.\\n\\nExercise'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T09:14:16.833706Z",
     "start_time": "2025-07-25T09:14:16.034811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenizer.chat_template = model_id\n",
    "chat = ChatHuggingFace(llm=llm)\n",
    "\n",
    "messages = \"<system>You are a helpful translator. Translate the user sentence to French.\\n <human>I love programming.\"\n",
    "# messages = [\n",
    "#     (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "#     (\"human\", \"I love programming.\"),\n",
    "# ]\n",
    "chat.invoke(messages)"
   ],
   "id": "314ba01667cc8466",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 9\u001B[0m\n\u001B[0;32m      4\u001B[0m messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<system>You are a helpful translator. Translate the user sentence to French.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m <human>I love programming.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# messages = [\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#     (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#     (\"human\", \"I love programming.\"),\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# ]\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m chat\u001B[38;5;241m.\u001B[39minvoke(messages)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:378\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    374\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    375\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m--> 378\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    379\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    380\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    381\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    382\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    383\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    384\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    385\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    386\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    387\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    388\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:963\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    955\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    956\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    960\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    961\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    962\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:782\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    781\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 782\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    783\u001B[0m                 m,\n\u001B[0;32m    784\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    785\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    786\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    787\u001B[0m             )\n\u001B[0;32m    788\u001B[0m         )\n\u001B[0;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1028\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1028\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1029\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1030\u001B[0m     )\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1032\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:577\u001B[0m, in \u001B[0;36mChatHuggingFace._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(answer)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 577\u001B[0m     llm_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_chat_prompt(messages)\n\u001B[0;32m    579\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_stream:\n\u001B[0;32m    580\u001B[0m         stream_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39m_stream(\n\u001B[0;32m    581\u001B[0m             llm_input, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    582\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:731\u001B[0m, in \u001B[0;36mChatHuggingFace._to_chat_prompt\u001B[1;34m(self, messages)\u001B[0m\n\u001B[0;32m    727\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLast message must be a HumanMessage!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    729\u001B[0m messages_dicts \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_chatml_format(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n\u001B[1;32m--> 731\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[0;32m    732\u001B[0m     messages_dicts, tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    733\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1621\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.apply_chat_template\u001B[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1619\u001B[0m     tokenizer_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1621\u001B[0m chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_chat_template(chat_template, tools)\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(conversation, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[0;32m   1624\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(conversation[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(conversation[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1625\u001B[0m ):\n\u001B[0;32m   1626\u001B[0m     conversations \u001B[38;5;241m=\u001B[39m conversation\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\data_science\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1743\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.get_chat_template\u001B[1;34m(self, chat_template, tools)\u001B[0m\n\u001B[0;32m   1741\u001B[0m         chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_template\n\u001B[0;32m   1742\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1744\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1745\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margument was passed! For information about writing templates and setting the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1746\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1747\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1748\u001B[0m         )\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chat_template\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
